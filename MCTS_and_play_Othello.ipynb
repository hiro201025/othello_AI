{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCTS and play Othello.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFHLOqDlwyunCvgpic1JOE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAcgFDNiX2-D"
      },
      "source": [
        "#code: utf-8\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "#import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#import matplotlib.patches as pat"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOWxgqH-YEqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a00dd9-6e38-4e91-9f18-e8f278e56d92"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "ROOT_PATH = '/content/gdrive/My Drive/Colab Notebooks'\n",
        "CUR_PATH = '/content/gdrive/My Drive/Colab Notebooks/Othello_AI'\n",
        "if ROOT_PATH not in sys.path:\n",
        "  sys.path.append(ROOT_PATH)\n",
        "if CUR_PATH not in sys.path:\n",
        "  sys.path.append(CUR_PATH)\n",
        "\n",
        "#for load params of trained model\n",
        "SL_MODEL_NAME = \"conv4_bn_mini\"\n",
        "SL_No = 40\n",
        "SL_PARAM_NAME = f\"SLpn_{SL_MODEL_NAME}_{SL_No}\"\n",
        "SL_PARAM_PATH = os.path.join(CUR_PATH, \"SLpn_params\", f\"{SL_PARAM_NAME}.pth\")\n",
        "\n",
        "#for rollout policy model\n",
        "RO_MODEL_NAME = \"conv1\" #\"conv2_bn\" #conv1: No.0, conv2_bn: No.15\n",
        "RO_No = 0 #15\n",
        "RO_PARAM_NAME = f\"SLpn_{RO_MODEL_NAME}_{RO_No}\"\n",
        "RO_PARAM_PATH = os.path.join(CUR_PATH, \"SLpn_params\", f\"{RO_PARAM_NAME}.pth\")\n",
        "\n",
        "#for value model\n",
        "VALUE_No = 6\n",
        "VALUE_PARAM_PATH = os.path.join(CUR_PATH, \"value_data\", f\"value_params_{VALUE_No}.pth\")"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W58f5Ji6YURU"
      },
      "source": [
        "import gym\n",
        "import tools"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ruy0pMyYYif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c3081d-67fe-44b2-f30a-38d63f1fde2f"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "SEED = 2021\n",
        "set_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #not need gpu?\n",
        "print(device)"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSCed1_KdqQe"
      },
      "source": [
        "white, black = (1, 2)\n",
        "node_print = False"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHQVhuxS_45N"
      },
      "source": [
        "#defining policy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vaoc65S_3m3"
      },
      "source": [
        "class Conv4_bn_mini(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Conv4_bn_mini, self).__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    \n",
        "    self.fc1 = nn.Linear(64*8*8, 64)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.relu(self.bn2(self.conv2(x)))\n",
        "    x = self.relu(self.bn3(self.conv3(x)))\n",
        "    x = self.relu(self.bn4(self.conv4(x)))\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7jt_pzgYf6Q"
      },
      "source": [
        "#making Rollout Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlC_neXPY-G8"
      },
      "source": [
        "conv2_bn : No 15 is the best model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sGqHGsvYfYQ"
      },
      "source": [
        "class Conv2_bn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Conv2_bn, self).__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    \n",
        "    self.fc1 = nn.Linear(128*8*8, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.relu(self.bn2(self.conv2(x)))\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgQjtRgXZ3kI"
      },
      "source": [
        "Conv1: conv(3->1) -> flatten -> linear(1*64->64)\n",
        "trained for 10 Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI0tXyLGZLW2"
      },
      "source": [
        "class Conv1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Conv1, self).__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm2d(1)\n",
        "    self.conv1 = nn.Conv2d(3, 1, 3, padding=1)\n",
        "    self.flatten = nn.Flatten()  \n",
        "    self.fc1 = nn.Linear(1*64, 64)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ1q9AKz055E"
      },
      "source": [
        "#making value net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5fua_LS03-l"
      },
      "source": [
        "class ValueNet_conv4(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ValueNet_conv4, self).__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    \n",
        "    self.fc1 = nn.Linear(64*8*8, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.relu(self.bn2(self.conv2(x)))\n",
        "    x = self.relu(self.bn3(self.conv3(x)))\n",
        "    x = self.relu(self.bn4(self.conv4(x)))\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeAMZWZQatI5"
      },
      "source": [
        "#making transformer of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJq8qJcUasqV"
      },
      "source": [
        "def to_feature(state, player):\n",
        "    state = state.astype('float32')\n",
        "    white_st = np.where(state == 3 - player, 1, 0)\n",
        "    black_st = np.where(state == player, 1, 0)\n",
        "    bw_st = np.where(state > 0, 1, 0)\n",
        "    state = np.stack([bw_st, white_st, black_st], axis = 0).reshape(-1, 3, 8, 8)\n",
        "    state = torch.tensor(state).float()\n",
        "    state = state.to(device)\n",
        "    return state\n",
        "\n",
        "\n",
        "def model_pred(model, state, player):\n",
        "    with torch.no_grad():\n",
        "        pred = model(to_feature(state, player))\n",
        "        pred = nn.Softmax(dim=1)(pred) #in all? or in capable to place?\n",
        "    return pred.to('cpu').numpy().copy()\n",
        "\n",
        "\n",
        "def model_pred_inposts(model, state, player, posts):\n",
        "    with torch.no_grad():\n",
        "        pred = model(to_feature(state, player))\n",
        "        pred_inposts = []\n",
        "        for pt in posts:\n",
        "            pred_inposts.append(pred[0][pt])\n",
        "        pred = nn.Softmax(dim=0)(torch.tensor(pred_inposts))\n",
        "    return pred.to('cpu').numpy().copy()"
      ],
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frbjX92ChV7M"
      },
      "source": [
        "#making battle class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpkdKc-xhVQQ"
      },
      "source": [
        "class TrainedModel(object):\n",
        "    def __init__(self, mode=\"max\"):\n",
        "        self.model = Conv4_bn_mini().to(device)\n",
        "        self.model.load_state_dict(torch.load(SL_PARAM_PATH, map_location=device))\n",
        "        self.model.eval()\n",
        "        self.mode = mode\n",
        "        self.name = \"SLpn_\"+self.mode\n",
        "\n",
        "\n",
        "    def get_action(self, state, player, posts, last_action):\n",
        "        pred = model_pred(self.model, state, player)\n",
        "        if len(posts) == 0:\n",
        "            action = -1\n",
        "        else:\n",
        "            preds = []\n",
        "            for pt in posts:\n",
        "                preds.append(pred[0][pt])\n",
        "            if self.mode == \"max\":\n",
        "                action = posts[preds.index(max(preds))]\n",
        "            elif self.mode == \"prob\":\n",
        "                action = random.choices(posts, weights=preds, k=1)\n",
        "            else:\n",
        "                action = posts[preds.index(max(preds))]\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "class RandomAction(object):\n",
        "    def __init__(self):\n",
        "        self.name = \"Random\"\n",
        "\n",
        "\n",
        "    def get_action(self, state, player, posts, last_action):\n",
        "        if len(posts) == 0:\n",
        "            action = -1\n",
        "        else:\n",
        "            action = random.choice(posts)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "class Human(object):\n",
        "    def __init__(self):\n",
        "        self.name = \"Human\"\n",
        "\n",
        "\n",
        "    def get_input(self, posts):\n",
        "        input_key = input(\"select a number at position. otherwise selecting action in random.\")\n",
        "        if input_key.isdecimal():\n",
        "            input_num = int(input_key)\n",
        "        else:\n",
        "            input_num = -1\n",
        "        \n",
        "        if input_num in posts:\n",
        "            action = input_num\n",
        "        else:\n",
        "            action = random.choice(posts)\n",
        "            print(\"selected action in random to wrong input.\")\n",
        "        print(f\"your action: {action}\")\n",
        "        return action\n",
        "\n",
        "\n",
        "    def get_action(self, state, player, posts, last_action):\n",
        "        if len(posts) == 0:\n",
        "            action = -1\n",
        "            print(\"your turn was skipped.\")\n",
        "        else:\n",
        "            action = self.get_input(posts)\n",
        "        return action"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj2hhtG0fYeU"
      },
      "source": [
        "#making node class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zud-SxuxfXrL"
      },
      "source": [
        "class Node(object):\n",
        "\n",
        "\n",
        "    def __init__(self, parent=None, prob=0):\n",
        "        self.parent = parent #parent node\n",
        "        self.child = {} #dictionaly in child node\n",
        "        self.n_visits = 0 #times visited node\n",
        "        self.Q = 0 #modified average value\n",
        "        self.u = prob #modeified prob by n and Cp\n",
        "        self.P = prob #prob\n",
        "\n",
        "\n",
        "    def is_parent(self):\n",
        "        return self.parent is None\n",
        "\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return len(self.child) == 0\n",
        "\n",
        "\n",
        "    def get_value(self):\n",
        "        return self.Q + self.u\n",
        "\n",
        "\n",
        "    def select(self, Cp):\n",
        "        for key in self.child:\n",
        "            self.child[key].u = self.child[key].U(Cp)\n",
        "            if node_print:\n",
        "                print(f\"key [{key:2d}] Q+u: {self.child[key].get_value():.4f}, \" +\n",
        "                 f\"prob: {self.child[key].P:.4f}, visits: {self.child[key].n_visits}, parent_visits: {self.n_visits}\")\n",
        "        return max(self.child.items(), key=lambda act_node: act_node[1].get_value()) #max of child.get_value in {[action]: child}\n",
        "\n",
        "\n",
        "    def U(self, Cp):\n",
        "        return Cp * self.P * np.sqrt(self.parent.n_visits) / (1 + self.n_visits)\n",
        "\n",
        "\n",
        "    def expand(self, action_prob):\n",
        "        for action, prob in action_prob:\n",
        "            if action not in self.child:\n",
        "                self.child[action] = Node(self, prob)\n",
        "\n",
        "\n",
        "    def update(self, leaf_Q):\n",
        "        self.n_visits += 1\n",
        "        self.Q += (leaf_Q - self.Q) / self.n_visits\n",
        "\n",
        "\n",
        "    def update_recursive(self, leaf_Q):\n",
        "        self.update(leaf_Q)\n",
        "        if not self.is_parent():\n",
        "            self.parent.update_recursive(leaf_Q)#player changed \n"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EHc8zdDYTJr"
      },
      "source": [
        "#making MCTS class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPMPnh5-YRGm"
      },
      "source": [
        "class MCTS(object):\n",
        "\n",
        "\n",
        "    def __init__(self, lmd=0.5, Cp=5, visit_thr=15, time_limit=10):\n",
        "        self.root = Node(None, 1.0)\n",
        "\n",
        "        #definig SLpn, value net and rollout model\n",
        "        self.policy_model = Conv4_bn_mini().to(device)\n",
        "        self.policy_model.load_state_dict(torch.load(SL_PARAM_PATH, map_location=device))\n",
        "        self.policy_model.eval()\n",
        "        self.value_model = ValueNet_conv4().to(device)\n",
        "        self.value_model.load_state_dict(torch.load(VALUE_PARAM_PATH, map_location=device))\n",
        "        self.value_model.eval()\n",
        "        self.rollout_model = Conv1().to(device) #Conv2_bn().to(device)\n",
        "        self.rollout_model.load_state_dict(torch.load(RO_PARAM_PATH, map_location=device))\n",
        "        self.rollout_model.eval()\n",
        "\n",
        "        #othello env for playout and rollout \n",
        "        self.root_env = gym.make(\"othello-v0\")\n",
        "        self.root_env.reset()\n",
        "        self.rollout_env = gym.make(\"othello-v0\")\n",
        "        self.rollout_env.reset()\n",
        "\n",
        "        self.lmd = lmd\n",
        "        self.Cp = Cp\n",
        "        self.visit_thr = visit_thr\n",
        "        self.time_limit = time_limit\n",
        "        self.name = \"MCTS\"\n",
        "\n",
        "\n",
        "    def policy_func(self, state, player, posts):\n",
        "        #pred = model_pred_inposts(self.policy_model, state, player, posts)\n",
        "        pred = model_pred(self.policy_model, state, player)\n",
        "        action_prob = []\n",
        "        if len(posts) > 0:\n",
        "            for pt in posts:\n",
        "                action_prob.append((pt, pred[0][pt]))\n",
        "        else:\n",
        "            action_prob.append((-1, 1))\n",
        "        return action_prob\n",
        "\n",
        "\n",
        "\n",
        "    def value_func(self, state, player):\n",
        "        with torch.no_grad():\n",
        "            value = self.value_model(to_feature(state, player))\n",
        "        return value.item()\n",
        "\n",
        "\n",
        "    def evaluate_rollout(self, state, me_player):\n",
        "        reward = self.rollout_env.board_reset(state, me_player)\n",
        "        player = me_player\n",
        "        while not self.rollout_env.done:\n",
        "            #self.rollout_env.render()\n",
        "            player = self.rollout_env.player\n",
        "            posts, _, _ = self.rollout_env.next_place\n",
        "            state = self.rollout_env.render(\"rgb_array\")\n",
        "            pred = model_pred(self.rollout_model, state, player)\n",
        "            if len(posts) == 0:\n",
        "                action = -1\n",
        "            else:\n",
        "                preds = []\n",
        "                for pt in posts:\n",
        "                    preds.append(pred[0][pt])\n",
        "                action = random.choices(posts, weights=preds, k=1)\n",
        "\n",
        "            _, action, next, reward, _, player = self.rollout_env.step(action)\n",
        "\n",
        "        winner = (player + (int(reward)+1)//2 )%2 + 1\n",
        "        winner *= int(reward**2)\n",
        "        if winner == me_player:\n",
        "            score = 1.0\n",
        "        elif winner == 0:\n",
        "            score = 0.0\n",
        "        else:\n",
        "            score = -1.0\n",
        "        return score\n",
        "\n",
        "\n",
        "    def playout(self, state, player, node):\n",
        "        #node = copy.copy(_node) #why copy?\n",
        "        self.root_env.board_reset(state, player)\n",
        "        if node.is_leaf():\n",
        "            if node.n_visits >= self.visit_thr: #expand step\n",
        "                posts, _, _ = self.root_env.next_place\n",
        "                action_prob = self.policy_func(state, player, posts)\n",
        "                node.expand(action_prob)\n",
        "                self.playout(state, player, node) #recursion\n",
        "            \n",
        "            else: #evaluate step\n",
        "                value = self.value_func(state, player)\n",
        "                score = self.evaluate_rollout(state.copy(), player) #win score [-1.0, 0.0, 1.0]\n",
        "                leaf_Q = (1-self.lmd) * value + self.lmd * score\n",
        "                node.update_recursive(leaf_Q)\n",
        "        \n",
        "        else: #select step\n",
        "            if self.root_env.done:\n",
        "                value = self.value_func(state, player)\n",
        "                #score = self.evaluate_rollout(state.copy(), player) #win score [-1.0, 0.0, 1.0]\n",
        "                #leaf_Q = (1-self.lmd) * value + self.lmd * score\n",
        "                node.update_recursive(value)\n",
        "            else:\n",
        "                action, node = node.select(self.Cp)\n",
        "                if node_print:\n",
        "                    print(f\"select action: {action}\")\n",
        "                _, action, next, reward, _, player = self.root_env.step(action)\n",
        "                self.playout(next, self.root_env.player, node)\n",
        "\n",
        "\n",
        "    def select_intime(self, state, player):\n",
        "        start = time.time()\n",
        "        elapsed = 0\n",
        "        while elapsed < self.time_limit:\n",
        "            self.playout(state.copy(), player, self.root)\n",
        "            last_time = elapsed\n",
        "            elapsed = time.time() - start\n",
        "            if node_print:\n",
        "                print(f\"elapsed: {elapsed:.2f} playout time: {elapsed - last_time:.2f}\")\n",
        "        action = max(self.root.child.items(), key=lambda act_node: act_node[1].n_visits)[0]\n",
        "        if node_print:\n",
        "            print(f\"detetmined action: {action}\")\n",
        "        return action\n",
        "\n",
        "\n",
        "    def update_root(self, action):\n",
        "        if action in self.root.child:\n",
        "            self.root = copy.copy(self.root.child[action])\n",
        "            self.root.parent = None\n",
        "        else:\n",
        "            self.root = Node(None, 1.0)\n",
        "\n",
        "\n",
        "    def get_action(self, state, player, posts, last_action):\n",
        "        if last_action > -2:\n",
        "            self.update_root(last_action)\n",
        "\n",
        "        if len(posts) > 1:\n",
        "            action = self.select_intime(state, player)\n",
        "        elif len(posts) == 1:\n",
        "            action = posts[0]\n",
        "        else:\n",
        "            action = -1\n",
        "        self.update_root(action)\n",
        "        \n",
        "        return action"
      ],
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWIH-l_1uu6W"
      },
      "source": [
        "#othello class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYoB1t1wuuiI"
      },
      "source": [
        "class OthelloPlay(object):\n",
        "    def __init__(self, white_model=Conv1, black_model=Conv1):\n",
        "        self.model = (white_model, black_model)\n",
        "        self.main_env = gym.make(\"othello-v0\")\n",
        "\n",
        "    \n",
        "    def _model_shift(self):\n",
        "        shift_model = (self.model[1], self.model[0])\n",
        "        self.model = shift_model\n",
        "\n",
        "\n",
        "    def _count_stones(self):\n",
        "        state = self.main_env.render(\"rgb_array\")\n",
        "        white_stones = np.sum(np.where(state == white, 1, 0))\n",
        "        black_stones = np.sum(np.where(state == black, 1, 0))\n",
        "        return (white_stones, black_stones)\n",
        "\n",
        "\n",
        "    def battle_loop(self, visible=True):\n",
        "        if visible:\n",
        "            print(f\"white: {self.model[0].name} vs. black: {self.model[1].name}\")\n",
        "        self.main_env.reset()\n",
        "        action = -2\n",
        "        while not self.main_env.done:\n",
        "            if visible:\n",
        "                self.main_env.render(\"human\")\n",
        "            player = self.main_env.player\n",
        "            posts, _, _ = self.main_env.next_place\n",
        "            state = self.main_env.render(\"rgb_array\")\n",
        "            action = self.model[player-1].get_action(state, player, posts, action)\n",
        "\n",
        "            _, action, next, reward, _, player = self.main_env.step(action)\n",
        "\n",
        "        if visible:\n",
        "            self.main_env.render(\"human\")\n",
        "        winner = (player + (int(reward)+1)//2 )%2 + 1\n",
        "        winner *= int(reward**2)\n",
        "\n",
        "        stones = self._count_stones()\n",
        "\n",
        "        if visible:\n",
        "            if winner == white:\n",
        "                print(f\"white: {self.model[0].name} win! white: {stones[0]} black: {stones[1]}\")\n",
        "            elif winner == black:\n",
        "                print(f\"black: {self.model[1].name} win! white: {stones[0]} black: {stones[1]}\")\n",
        "            else:\n",
        "                print(f\"draw! white: {stones[0]} black: {stones[1]}\")\n",
        "\n",
        "        return winner, stones\n",
        "\n",
        "\n",
        "    def testplay(self, turn=\"one\"):\n",
        "        if turn == \"both\":\n",
        "            self.battle_loop(visible=True)\n",
        "            self._model_shift()\n",
        "            self.battle_loop(visible=True)\n",
        "        else:\n",
        "            self.battle_loop(visible=True)\n",
        "        \n",
        "\n",
        "\n",
        "    def evalplay(self, n=10):\n",
        "        for _ in range(2):\n",
        "            winner_cnt = [0, 0, 0] #draw, white, black\n",
        "            winner_stones = [0, 0, 0]\n",
        "            for _ in range(n):\n",
        "                winner, stones = self.battle_loop(visible=False)\n",
        "                winner_cnt[winner] += 1\n",
        "                winner_stones[winner] += max(stones) / (64 * n)\n",
        "            \n",
        "            print(f\"white: {self.model[0].name} black: {self.model[1].name}\")\n",
        "            print(f\"win count [{n} battles]\")\n",
        "            print(f\"white: {winner_stones[1]}/{winner_cnt[1]} black: {winner_stones[2]}/{winner_cnt[2]} draw: {winner_stones[0]}/{winner_cnt[0]}\")\n",
        "            self._model_shift()"
      ],
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfe578iY1pgO"
      },
      "source": [
        "#making battle classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIZcdCQU1obh"
      },
      "source": [
        "SLpn = TrainedModel() #conv4_bn_mini\n",
        "Random = RandomAction() #random\n",
        "You = Human() #input key\n",
        "mcts = MCTS(Cp=5, visit_thr=15, time_limit=10) #mcts"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWqb35YMgsFq"
      },
      "source": [
        "#Play Othello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOO-OijHyfL-",
        "outputId": "22c3f719-de65-4db6-e2e7-c2a4a380f87d"
      },
      "source": [
        "#OthelloPlay(SLpn, mcts).testplay(\"one\")\n",
        "OthelloPlay(SLpn, mcts).evalplay(1)"
      ],
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "white: SLpn_max black: MCTS\n",
            "win count [1 battles]\n",
            "white: 0/0 black: 0/0 draw: 0.5/1\n",
            "white: MCTS black: SLpn_max\n",
            "win count [1 battles]\n",
            "white: 0/0 black: 0.703125/1 draw: 0/0\n"
          ]
        }
      ]
    }
  ]
}